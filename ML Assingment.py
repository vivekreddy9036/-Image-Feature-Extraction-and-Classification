# -*- coding: utf-8 -*-
"""ML Assingment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vlP3f9E__fPfu_DYJAr0dXlocQ1dZcvP
"""

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
import warnings
from skimage.feature import hog, local_binary_pattern
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess
from sklearn.model_selection import train_test_split
import tensorflow as tf

# Suppress warnings
warnings.filterwarnings("ignore")

# Load CIFAR-10 dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

# Combine train and test to sample 5000 images
X_total = np.concatenate((x_train, x_test), axis=0)
y_total = np.concatenate((y_train, y_test), axis=0)

# Randomly select 5000 samples
indices = np.random.choice(len(X_total), 5000, replace=False)
X_sampled = X_total[indices]
y_sampled = y_total[indices]

# Split into 70% train (3500) and 30% test (1500)
X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.3, random_state=42, stratify=y_sampled)

# Normalize images
X_train = X_train.astype("float32") / 255.0
X_test = X_test.astype("float32") / 255.0

# Feature extraction functions
def extract_hog_features(image):
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    return hog(gray, pixels_per_cell=(8, 8), cells_per_block=(2, 2), feature_vector=True)

def extract_lbp_features(image):
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    lbp = local_binary_pattern(gray, P=24, R=3, method="uniform")
    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, 27), range=(0, 26))
    hist = hist.astype("float") / (hist.sum() + 1e-6)  # Normalize
    return hist

# Extract features for training and testing
def extract_features(dataset, feature_extractor):
    return np.array([feature_extractor(img) for img in dataset])

import pandas as pd
from sklearn.metrics import accuracy_score, classification_report

# Train and evaluate function with result collection
def train_and_evaluate(feature_name, feature_extractor, classifier_type, results):
    print(f"\nEvaluating {feature_name} Features with {classifier_type}...")

    X_train_feat = extract_features(X_train, feature_extractor)
    X_test_feat = extract_features(X_test, feature_extractor)

    scaler = StandardScaler()
    X_train_feat = scaler.fit_transform(X_train_feat)
    X_test_feat = scaler.transform(X_test_feat)

    classifiers = {
        "RandomForest": RandomForestClassifier(n_estimators=100, random_state=42),
        "LogisticRegression": LogisticRegression(max_iter=1000),
        "KNN": KNeighborsClassifier(n_neighbors=5),
        "DecisionTree": DecisionTreeClassifier()
    }

    clf = classifiers[classifier_type]
    clf.fit(X_train_feat, y_train.ravel())
    y_pred = clf.predict(X_test_feat)

    acc = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, output_dict=True)

    results.append({
        "Feature": feature_name,
        "Classifier": classifier_type,
        "Accuracy": acc,
        "Precision": report["weighted avg"]["precision"],
        "Recall": report["weighted avg"]["recall"],
        "F1-Score": report["weighted avg"]["f1-score"]
    })

# Collect results
results = []
for classifier in ["RandomForest", "LogisticRegression", "KNN", "DecisionTree"]:
    train_and_evaluate("HOG", extract_hog_features, classifier, results)
    train_and_evaluate("LBP", extract_lbp_features, classifier, results)

# Convert results to DataFrame and print
df_results = pd.DataFrame(results)
print("\nFinal Results Table:")
print(df_results)

import pandas as pd
from sklearn.metrics import accuracy_score, classification_report

# Store results
results_dl = []

for model_name, (base_model, preprocess_func) in models.items():
    print(f"\nExtracting Features using {model_name}...")
    model = Model(inputs=base_model.input, outputs=base_model.output)

    X_train_dl = extract_deep_features(model, X_train, preprocess_func)
    X_test_dl = extract_deep_features(model, X_test, preprocess_func)

    scaler = StandardScaler()
    X_train_dl = scaler.fit_transform(X_train_dl)
    X_test_dl = scaler.transform(X_test_dl)

    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_train_dl, y_train.ravel())
    y_pred_dl = clf.predict(X_test_dl)

    acc = accuracy_score(y_test, y_pred_dl)
    report = classification_report(y_test, y_pred_dl, output_dict=True)

    # Store results for the final table
    results_dl.append({
        "Model": model_name,
        "Accuracy": acc,
        "Precision": report["weighted avg"]["precision"],
        "Recall": report["weighted avg"]["recall"],
        "F1-Score": report["weighted avg"]["f1-score"]
    })

    print("Accuracy:", acc)
    print(classification_report(y_test, y_pred_dl))

# Convert results to DataFrame and print final table
df_results_dl = pd.DataFrame(results_dl)
print("\nFinal Results Table:")
print(df_results_dl)